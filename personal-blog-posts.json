{
  "posts": [
    {
      "type": "Personal",
      "id": "a-review-of-01-ai",
      "title": "A Review of 01.AI & Yi-Large - Developing Useful & Efficient AI for All",
      "date": "September 26th, 2024",
      "author": "Nathan Beaudoin",
      "summary": "A review of both a model, as well as a unique strategy for a long-term plan in the unpredictable market of Artificial Intelligence.",
      "content": "<br><h2>Brief History</h2> In late March of 2023, a new AI lab was formed called 01.AI. A little over a year later, they have released multiple large language models to the open-source community, released their flagship model, Yi-Large, to the world through their API, and is now valued at one billion USD. Over the past month, I have been experimenting with their Yi-Large model, daily driving it to see how it performs against other leading models. I must say the performance is quite remarkable, even more so when factoring in how 01.AI got to this point in a very short time.<br><br>01.AI is a Chinese-based AI lab that was started by Kai-Fu Lee, a Taiwanese businessman and computer scientist, with the vision statement to <a href='https://www.01.ai/' target='_blank'>'make AGI accessible and beneficial to everyone'</a>. Kai-Fu Lee went to Carnegie Mellon University where he worked on different topics within the Machine Learning field, and completed his doctoral dissertation in 1988 on the SPHINX system, a large-vocabulary speaker-independent continuous speech recognition system. He has jumped around from Apple, to Silicon Graphics, to Microsoft, and Google as well spearheading both Google China as well as helping launch Microsoft Research Asia. He has been in the machine learning space for a long time, and his knowledge and confidence for the future of AI shows that expertise. Kai-Fu Lee has always believed that AI as a whole has the potential to help humanity as a whole, but did not know when this transition would begin, so it took him by surprise when ChatGPT launched and became the fastest growing consumer application in history. Because of this, he realized that things were beginning to shift in this direction sooner than he thought, and took it upon himself to help push progress, especially within China where access to leading models is limited. <br><br>Kai-Fu Lee soon launched 01.AI to build a large language model for the Chinese market, as well as help build out an ecosystem for developers and clients around the world to integrate powerful but affordable models into their own applications through open models, as well as though a flexible API. Kai-Fu Lee expressed plans to develop a 100-billion-plus parameter model that will have performance on par with GPT-4, but at the time, this was only a vision that would soon be realized a year later.<br><br>The first models released by 01.AI was Yi-6B & Yi-34B, both open-weight with a commercial license in November of 2023 (Yi-9B later released in March). These models were impressively released only seven months after the company was founded, with strong performance in both the Chinese & English languages. Kai-Fu Lee noted that these models were released openly to 'give back' to the open-source community as a sign of graditude. These models also helped in different ways to determine how to develop Yi-Large in terms of scaling data mixures, and much more. A few days later, they released long context variants of the models, boasting up to a 200K context length. After further testing months later by independent developers, the usable context length of the models is around 80-100K, which is still quite impressive as many models at the time with long context lengths had only a fraction of the usable context length. Later in January, the Yi-VL open-weight models were released, which added experimental vision capabilities to the open-weight models.<br><br>In May of 2024, upgraded versions of the original open-weight Yi models were released under the family name of Yi-1.5. These models were continued pre-trained versions of the original Yi models with additional code and math data to improve the performance of the models in those areas, making them more powerful for developers to efficiently experiment with on their own machines.<br><br>Finally in May as well, Yi-Large was released to the world with 100+ billion parameters, having beyond impressive performance for how quickly it was developed, beating GPT-4 on many benchmarks.<br><br><a href='./assets/blogs/personal/yi-review/benchmarks.jpg' target='_blank'><img src='./assets/blogs/personal/yi-review/benchmarks.jpg' alt='Yi-Large benchmarks' class='blog-image'></a><br>On the LMSYS leaderboard when the model was first tested, the model impressively outperformed many models from top AI labs in the Overall ranking of the leaderboard, being in the top 10 of models. In the Chinese category though, Yi-Large smashes other models, achieving SOTA performance, being in the top 3 of models when it came to Chinese queries.<br><br><a href='./assets/blogs/personal/yi-review/lmsys.jpg' target='_blank'><img src='./assets/blogs/personal/yi-review/lmsys.jpg' alt='Yi-Large LMSYS benchmarks' class='blog-image'></a><br>For how capable this model was, as well as factoring in how quickly 01.AI came to be, I needed to test this model out myself to see where its strengths and weaknesses lie in relation to top models of more established labs.<br><br><br><br><h2>Daily Driving this Model</h2> In mid-July, I was given a free API key by a member of 01.AI's DevRel team, which I am very thankful for, and it is what originally gave me the idea to solely use this model for an entire month (for the most part), as I would not need to pay for it and could use it worry free. This would not have been an issue if I did have to pay for it, as the pricing of this model is quite competitive for its power at $3 for one million input tokens and $3 for one million output tokens.<br><br>I am not going to tell you about how the model is powerful or how it is on a similar scale to other types of models, as you have heard that a million times. The fact of the matter is this model is on the level of an above average text-only large language model on the global stage when it was first released. Now after almost two months of the model being out, a massive influx of powerful large language models have hit the scene such as GPT-4o mini and Mistral Large 2, making this model be around this level as an average text-only large language model.<br><br>Why even talk about this model if it is just another model among an ever growing sea of of models? Wellâ€¦ because there is a few notable things about this model that I have not seen out of similar sized models, only really appearing in models that are cutting edge, while being smaller than those sizes as well as being developed and released within a relatively quick time span.<br><br><br>Here are some of the things I found with the model that are notable to talk about:<br><br><h3>1. Strong coding abilities with friendlier logical language</h3> Like larger models like Claude 3 Opus, Yi-Large's wording when giving code back to the user is stuff like 'this should' or 'this issue is likely', which I like a lot as it shows that the model is not saying that something it gives out is completely right. It frames it as something the user can then try, and is not saying that it WILL work right away, unlike some models of similar size to Yi-Large. I see this ability in models like Claude Opus and GPT-4-Turbo, so in that regard, Yi-Large has friendlier language when it comes to delivering solutions to logical understanding in code and other areas. Using Yi-Large when it came to coding was a more enjoyable and less frustrating experience compared to some more powerful models like GPT-4-Turbo, as its subtle language and wording while conversing with you makes me feel less annoyed when it makes mistakes at times.<br><h3>2. An expressive and creative powerhouse when it comes to uniqueness of elements within writing</h3>Yi-Large feels like it actually has more of a writing 'soul' compared to models like GPT-4-0314, GPT-4-Turbo, and Gemini 1.5 Flash. For example I told each model to write a short horror story with a small general description and those three models gave more bland responses, or they took some details I said like 'horrific' and just made it a descriptor word in the story, while Yi-Large gave stories that felt much less bland and did not just take words I told it and use them as descriptor words; it actually included and followed what I wanted. This was over 10 different generations, and Yi-Large showed it has quite a creative writing capacity that was not just a one off thing. The model has a lot of creative depth to it similar to when Claude 3 Opus & Sonnet first released back in February.<br><h3>3. Solid and reliable instruction following abilities</h3>When testing this model to follow instructions, Yi-Large performed exceptionally well and gave me performance that did not fluctuate across a wide range of tasks I had thrown at it. Throughout all of them, the model was able to follow complex instructions I gave it at both short and long contexts. The instruction following abilities are similar to Claude 3 Opus when it first launched, which I find impressive for its size, and of course, how quickly it was developed.<br><h3>4. Multilingual capabilities</h3>While not tested in depth by myself as I only speak English, when looking at LMSYS leader board, this model shines brightly in Chinese, which makes sense as this model was built with a heavy priority of being powerful for the Chinese market and Chinese customers. Not only that, this model performs highly in all other languages as well, holding a flame to models like Claude 3 Opus and GPT-4-Turbo. This is important to note as there are models such as LLaMA-3-70B where they have strong English performance, but suffer greatly in different languages, having results that can jump up or down the leader board from one multilingual category to another. Even though Yi-Large is only a bit bigger than LLaMA-3-70B, this gives it the room to have developed a solid language foundation to have solid performance across languages. This in turn opens it up to be useful to a much larger market of people outside of English and Chinese speaking markets.<br><br>These four things are what I noticed when researching about Yi-Large, as well as testing it throughout the month. They are in my opinion what helps set this model apart from other similarly sized models, and what makes this model a very versatile text-only large language model to a lot of different people and developers in different markets around the world.<br><br>The one major thing that I believe makes this model special among other models in the space is not the model itself actually, but the ecosystem, developer-friendly documentation, as well as tools that 01.AI is putting a lot of focus into. 01.AI have taken an approach to providing very detailed documentation, tools, examples, and more in how to integrate their models into different applications easily. Instead of focusing on trying to make the most State-Of-The-Art model, or trying to fight to have the cheapest models, they focus on trying to have a diverse range of powerful and affordable models all in one place with heavy documentation to help developers and companies easily and effectively integrate and get the most out of these models. This is one of the most promising long term plans for an AI lab I have seen as generative AI is genuinely useful in a variety of domains, and when integrated right, can be quite helpful, safe, and efficient for developers and companies who invest into using these models.<br><br><br>One of the main things that is interesting to think about is how can a model that is most likely smaller than GPT-3 perform on the level of GPT-4?<br><br>Data quality, that's mainly it.<br><br>In the <a href='https://arxiv.org/abs/2403.04652' target='_blank'>Yi Open Foundational Models paper</a>, a lot of detail and thought was put into data processing as it is well known that the quality of data used in the training of models can reflect in how it performs. We can use the chart below to estimate what they did for Yi-Large.<br><br><br><br><a href='./assets/blogs/personal/yi-review/data-pie-chart.jpg' target='_blank'><img src='./assets/blogs/personal/yi-review/data-pie-chart.jpg' alt='Yi open models data pie chart' class='blog-image'></a><br><br>They put a lot of work in what data to include, rules for filtering, deduplication, and much more to create a training dataset that is diverse, yet high quality. This dataset in contrast to the original GPT-3 dataset is of much higher quality and size, allowing for a model trained on this dataset to exhibit properties of much larger models, like GPT-4, even though it most likely has less parameters than GPT-3 itself.<br><br>Because of this focus on high quality but diverse data, this model completely surpasses the original GPT-4 in the real world. This is beyond impressive since this model was completed in only a year's time from scratch by leveraging the work that has come prior to them, as well as leveraging the most out of their hardware.<br><br>01.AI actually has provided a bit of details into their training cluster and how Yi-Large was trained in their <a href='https://01-ai.github.io/blog.html?post=zh/2024-04-30-%E9%9B%B6%E4%B8%80%E4%B8%87%E7%89%A9%E9%9D%A2%E5%90%91%E4%B8%87%E5%8D%A1%E9%9B%86%E7%BE%A4%E7%9A%84%20AI%20Infra%20%E5%BB%BA%E8%AE%BE.md' target='_blank'>blog</a>. This is something you don't usually see from an AI lab in giving a detailed look into their training pipeline, but it goes to show the computational power it takes to train a model of this caliber, and why it is so crucial to focus on reducing the needed compute to train such a model, such as focusing on the quality of data rather than training a larger model. <a href='https://01-ai.github.io/blog.html?post=zh/2024-07-23-X-Chip%E4%B8%8E%E4%B8%BB%E6%B5%81%E8%8A%AF%E7%89%87%E7%9A%84%E6%8E%A8%E7%90%86%E5%9C%BA%E6%99%AF%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94.md' target='_blank'>According to their blog</a>, they trained Yi-Large in FP8, which was not a good strategy until very recently with advancements in hardware. This example of a technique used to train Yi-Large cheaper and faster than using normal FP16 or BF16.<br><br><br><br><br><h2>Final Thoughts</h2>Yi-Large & 01.AI are both unique in the AI space from how they quickly and efficiently trained a GPT-4 class model within a year, released open-weight models to developers, and providing detailed documentation and examples of how to develop and integrate their models, both big and small. Their focus on user experience instead of trying to provide the most SOTA or cheapest model while operating in a constrained training environment is something I am very glad to see, and show me more common among AI labs.<br><br>Thank you very much to <a href='https://huggingface.co/lorinma' target='_blank'>Nuo Ma</a>, <a href='https://www.linkedin.com/in/yu-liu-9968ba169/' target='_blank'>Yu Liu</a>, and <a href='https://x.com/Chen_01AI' target='_blank'>Chen</a> from 01.AI's DevRel team for giving me free API access to test their model as well as being very helpful overall!<br><br><br><br><small><strong>Sources</strong><br><a href='https://www.01.ai/' target='_blank'>Yi-Large Benchmark Table</a><br><a href='https://techcrunch.com/2023/11/05/valued-at-1b-kai-fu-lees-llm-startup-unveils-open-source-model/' target='_blank'>Techcrunch Article</a><br><a href='https://01-ai.github.io/index.html' target='_blank'>01.AI's Blog</a><br><a href='https://discord.gg/32CMh2yjTD' target='_blank'>01.AI's Discord</a></small>",
      "imageUrl": "./assets/blogs/personal/yi-review/main.jpg"
    }
  ]
}